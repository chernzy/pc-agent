{
    "models": {
        "supported": [
            {
                "name": "Mistral 7B int4",
                "id": "mistral_model",
                "ngc_model_name": "nvidia/llama/mistral-7b-int4-chat:1.2",
                "is_downloaded_required": true,
                "downloaded": true,
                "is_installation_required": true,
                "setup_finished": true,
                "min_gpu_memory": 8,
                "should_show_in_UI": true,
                "prerequisite": {
                    "checkpoints_files": [
                        "config.json",
                        "rank0.safetensors",
                        "license.txt"
                    ],
                    "tokenizer_ngc_dir": "mistral7b_hf_tokenizer",
                    "tokenizer_files": {
                        "config": "config.json",
                        "tokenizer": "tokenizer.json",
                        "model": "tokenizer.model",
                        "tokenizer_config": "tokenizer_config.json"
                    },
                    "checkpoints_local_dir": "model_checkpoints",
                    "tokenizer_local_dir": "tokenizer",
                    "engine_build_command": "trtllm-build --checkpoint_dir %checkpoints_local_dir% --output_dir %engine_dir% --gpt_attention_plugin float16 --gemm_plugin float16 --max_batch_size 1 --max_input_len 7168 --max_output_len 1024 --context_fmha=enable --paged_kv_cache=disable --remove_input_padding=disable",
                    "engine_dir": "engine"
                },
                "metadata": {
                    "engine": "rank0.engine",
                    "max_new_tokens": 1024,
                    "max_input_token": 7168,
                    "temperature": 0.1,
                    "prompt_template": "Default"
                },
                "model_info": "The Mistral-7B is a instruct fine-tuned text generation model | <a href='https://github.com/mistralai/mistral-src/blob/main/LICENSE'> License </a>",
                "model_license": "<a href='https://github.com/mistralai/mistral-src/blob/main/LICENSE'> License </a>",
                "model_size": "4GB"
            },
            {
                "name": "Llama2 13B int4",
                "id": "llaam2_model",
                "ngc_model_name": "nvidia/llama/llama2-13b:1.5",
                "is_downloaded_required": true,
                "downloaded": false,
                "is_installation_required": true,
                "setup_finished": false,
                "min_gpu_memory": 16,
                "should_show_in_UI": false,
                "prerequisite": {
                    "checkpoints_files": [
                        "config.json",
                        "rank0.safetensors",
                        "README.txt",
                        "license.txt"
                    ],
                    "tokenizer_ngc_dir": "llama13_hf_tokenizer",
                    "tokenizer_files": {
                        "config": "config.json",
                        "tokenizer": "tokenizer.json",
                        "model": "tokenizer.model",
                        "tokenizer_config": "tokenizer_config.json"
                    },
                    "checkpoints_local_dir": "model_checkpoints",
                    "tokenizer_local_dir": "tokenizer",
                    "engine_build_command": "trtllm-build --checkpoint_dir %checkpoints_local_dir% --output_dir %engine_dir% --gpt_attention_plugin float16 --gemm_plugin float16 --max_batch_size 1 --max_input_len 3072 --max_output_len 1024 --context_fmha=enable --paged_kv_cache=disable --remove_input_padding=disable",
                    "engine_dir": "engine"
                },
                "metadata": {
                    "engine": "rank0.engine",
                    "max_new_tokens": 1024,
                    "max_input_token": 7168,
                    "temperature": 0.1,
                    "prompt_template": "Default"
                },
                "model_info": "LlaMa 2 is a large language AI model capable of generating text and code in response to prompts | <a href='https://ai.meta.com/llama/license/'> License </a>",
                "model_license": "<a href='https://ai.meta.com/llama/license/'> License </a>",
                "model_size": "6.8GB"
            },
            {
                "name": "ChatGLM 3 6B int4 (Supports Chinese)",
                "id": "chatglm3_model",
                "ngc_model_name": "nvidia/chatglm3-6b-chat-int4:1.0",
                "is_downloaded_required": true,
                "downloaded": false,
                "is_installation_required": true,
                "setup_finished": false,
                "min_gpu_memory": 8,
                "should_show_in_UI": true,
                "prerequisite": {
                    "checkpoints_files": [
                        "config.json",
                        "rank0.safetensors",
                        "license.txt"
                    ],
                    "tokenizer_ngc_dir": "Tokenizer",
                    "tokenizer_files": {
                        "config": "config.json",
                        "model": "tokenizer.model",
                        "tokenizer_config": "tokenizer_config.json"
                    },
                    "checkpoints_local_dir": "model_checkpoints",
                    "tokenizer_local_dir": "tokenizer",
                    "engine_build_command": "trtllm-build --checkpoint_dir %checkpoints_local_dir% --output_dir %engine_dir% --gemm_plugin float16 --max_batch_size 1 --max_input_len 7168 --max_output_len 1024",
                    "engine_dir": "engine"
                },
                "metadata": {
                    "engine": "rank0.engine",
                    "max_new_tokens": 1024,
                    "max_input_token": 7168,
                    "temperature": 0.1
                },
                "model_info": "ChatGLM-6B is an open bilingual language model based on General Language Model framework, with 6.2 billion parameters | <a href= 'https://huggingface.co/THUDM/chatglm3-6b/blob/main/MODEL_LICENSE'> License </a>",
                "model_license": "<a href= 'https://huggingface.co/THUDM/chatglm3-6b/blob/main/MODEL_LICENSE'> License </a>",
                "model_size": "3.8GB"
            },
            {
                "name": "Gemma 7B int4",
                "id": "Gemma_model",
                "ngc_model_name": "nvidia/llama/gemma-7b-int4-rtx:1.1",
                "is_downloaded_required": true,
                "downloaded": false,
                "is_installation_required": true,
                "setup_finished": false,
                "min_gpu_memory": 16,
                "should_show_in_UI": false,
                "prerequisite": {
                    "checkpoints_files": [
                        "config.json",
                        "rank0.safetensors",
                        "Prohibited_use_policy.txt",
                        "license.txt",
                        "Notice.txt"
                    ],
                    "tokenizer_ngc_dir": "Gemma7b_hf_tokenizer",
                    "tokenizer_files": {
                        "vocab_file": "tmp_vocab.model"
                    },
                    "checkpoints_local_dir": "model_checkpoints",
                    "vocab_local_dir": "tokenizer",
                    "engine_build_command": "trtllm-build --checkpoint_dir %checkpoints_local_dir% --gemm_plugin float16 --gpt_attention_plugin float16 --max_batch_size 1 --max_input_len 4096 --max_output_len 1024 --output_dir %engine_dir%",
                    "engine_dir": "engine"
                },
                "metadata": {
                    "engine": "rank0.engine",
                    "max_new_tokens": 1024,
                    "max_input_token": 7168,
                    "temperature": 0.1
                },
                "model_info": "Gemma-7B is a 7B parameter model from Gemma family of models from Google | <a href= 'https://ai.google.dev/gemma/terms'> License </a>",
                "model_license": "<a href= 'https://ai.google.dev/gemma/terms'> License </a>",
                "model_size": "6.6GB"
            },
            {
                "name": "CLIP",
                "id": "clip_model",
                "hf_model_name": "openai/clip-vit-large-patch14-336",
                "is_downloaded_required": true,
                "downloaded": false,
                "download_link": "https://huggingface.co/openai/clip-vit-large-patch14-336/resolve/main",
                "is_installation_required": false,
                "setup_finished": false,
                "min_gpu_memory": 8,
                "should_show_in_UI": true,
                "prerequisite": {
                    "checkpoints_files": [
                        "README.md",
                        "config.json",
                        "merges.txt",
                        "preprocessor_config.json",
                        "pytorch_model.bin",
                        "special_tokens_map.json",
                        "tokenizer.json",
                        "tokenizer_config.json",
                        "vocab.json"
                    ],
                    "checkpoints_local_dir": "clip_model"
                },
                "metadata": {},
                "model_info": "CLIP is a multi-modal vision and language model used for image-text similarity and for zero-shot image classification | <a href='https://github.com/openai/CLIP/blob/main/LICENSE'> License </a>",
                "model_license": "<a href='https://github.com/openai/CLIP/blob/main/LICENSE'> License </a>",
                "model_size": "1.5GB"
            }
        ],
        "selected": "Mistral 7B int4",
        "enable_asr": true,
        "supported_asr": [
            {
                "name": "Whisper Medium Int8",
                "installed": true,
                "metadata": {
                    "encoder_engine": "whisper_encoder_float16_tp1_rank0.engine",
                    "decoder_engine": "whisper_decoder_float16_tp1_rank0.engine",
                    "model_path": "model\\whisper\\whisper_medium_int8_engine",
                    "assets_path": "model\\whisper\\whisper_assets"
                }
            }
        ]
    },
    "sample_questions": [
        {
            "query": "How does NVIDIA ACE generate emotional responses?"
        },
        {
            "query": "What is Portal prelude RTX?"
        },
        {
            "query": "What is important about Half Life 2 RTX?"
        },
        {
            "query": "When is the launch date for Ratchet & Clank: Rift Apart on PC?"
        }
    ],
    "sample_questions_chinese": [
        {
            "query": "NVIDIA ACE是如何生成富有情感的回复的？"
        },
        {
            "query": "传送门：序曲是什么？"
        },
        {
            "query": "半条命 2 RTX版有什么重要意义？"
        },
        {
            "query": "瑞奇与叮当：时空跳转何时会在PC平台发布？"
        }
    ],
    "sample_questions_clip": [
        {
            "query": "Pictures of bicycles"
        },
        {
            "query": "Pictures of toys"
        },
        {
            "query": "Pictures of dinosaurs"
        },
        {
            "query": "Pictures of computer"
        }
    ],
    "dataset": {
        "sources": [
            "directory",
            "nodataset"
        ],
        "selected": "directory",
        "path": "dataset",
        "path_chinese": "chinese_dataset",
        "path_clip": "images_dataset",
        "isRelative": true
    },
    "strings": {
        "directory": "Folder Path",
        "nodataset": "AI model default"
    }
}